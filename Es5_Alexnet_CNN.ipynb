{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkShlkUqu963tqmegYanKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackyjack00/Machine_Learning_Deep_Learning_LAB/blob/main/Es5_Alexnet_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZrdD4qCCF6z5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crate a data loader for the cifar10 dataset"
      ],
      "metadata": {
        "id": "CWjz5YsKJ4PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform = list() \n",
        "  transform.append(transforms.Resize((227,227)))\n",
        "  #Converts Numpy to Pytorch Tensor\n",
        "  transform.append( transforms.ToTensor() )\n",
        "  # Normalizes the Tensors between [-1, 1]\n",
        "  transform.append( transforms.Normalize(mean=[0.5], std=[0.5]) )\n",
        "  #combine the transformations defined and stucked in the list\n",
        "  transform = transforms.Compose(transform)\n",
        " \n",
        "  #Create train and validation splits of CIFAR10\n",
        "  full_training_data = torchvision.datasets.CIFAR10('./data', train=True, transform=transform, download=True)\n",
        "  test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform, download=True)\n",
        "  #define dimensions of split\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "  #generate splits\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders \n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "pZ_TiwqYJlNq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build AlexNet architechture from scratch"
      ],
      "metadata": {
        "id": "xxjM8I0AjOXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self , num_classes):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "            torch.nn.BatchNorm2d(96),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    \n",
        "    self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    \n",
        "    self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.BatchNorm2d(384),\n",
        "            torch.nn.ReLU())\n",
        "    \n",
        "    self.layer4 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.BatchNorm2d(384),\n",
        "            torch.nn.ReLU())\n",
        "    \n",
        "    self.layer5 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.BatchNorm2d(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    \n",
        "    self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(0.5),\n",
        "            torch.nn.Linear(9216, 4096),\n",
        "            torch.nn.ReLU())\n",
        "    \n",
        "    self.fc1 = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(0.5),\n",
        "            torch.nn.Linear(4096, 4096),\n",
        "            torch.nn.ReLU())\n",
        "    \n",
        "    self.fc2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(4096, num_classes))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #convo layers\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = self.layer5(out)\n",
        "\n",
        "    #flat the output, from ocnvo to fully connected\n",
        "    out = out.reshape(out.size(0), -1)\n",
        "    \n",
        "    #fully connected\n",
        "    out = self.fc(out)\n",
        "    out = self.fc1(out)\n",
        "    out = self.fc2(out)    \n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "2Z5oMklxgcCk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some functions to set LossFunction and Optimizer"
      ],
      "metadata": {
        "id": "oxIoTSjVjGsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use SGD optimizer for weights update\n",
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr,  weight_decay=wd, momentum=momentum)\n",
        "  return optimizer\n",
        "\n",
        "#Use CrossEntropy as Loss function\n",
        "def get_loss_function():\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "  return loss_function"
      ],
      "metadata": {
        "id": "I424OKQyil04"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some parameters settings"
      ],
      "metadata": {
        "id": "zKv9avFdbiVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "device='cuda:0'\n",
        "learning_rate=0.01\n",
        "weight_decay=0.000001\n",
        "momentum=0.9\n",
        "epochs=50"
      ],
      "metadata": {
        "id": "UzXnhrC8bcgc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "-qmB1lyGb3GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = get_data(batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5140ItjlbyCz",
        "outputId": "5d2d5979-8b24-4ff6-ff73-7fcfacd41b4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the train and test operations"
      ],
      "metadata": {
        "id": "F9S-bN7Sn0bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data_loader, optimizer, loss_function, device=\"cuda:0\"):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.train() \n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    #Load data into GPU\n",
        "    net = net.to(device)\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device) \n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = net(inputs) \n",
        "    \n",
        "    # Apply the loss\n",
        "    loss = loss_function(outputs,targets) \n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward() \n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Reset optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "def test(net, data_loader, cost_function, device=\"cuda:0\"):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  net.train() \n",
        "  # memory efficient trick\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU \n",
        "      net = net.to(device)\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "      _, predicted = outputs.max(1)\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "LMuVkvEekm4C"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main train test scope"
      ],
      "metadata": {
        "id": "2jGc80VNnpMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.dataset.dataset.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uMvIsbez7Ie",
        "outputId": "5ccf7a47-b86d-4a01-fb03-ce7342032926"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = LeNet( num_classes = 10 )\n",
        "optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "loss_function = get_loss_function() \n",
        "for e in range(epochs):\n",
        "  train_loss, train_accuracy = train(net, train_loader, optimizer, loss_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, loss_function)\n",
        "  print('Epoch: {:d}'.format(e+1))\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss,  train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss,  val_accuracy))\n",
        "  print(\"-----------------------------------------------------\")"
      ],
      "metadata": {
        "id": "0t_uVEnnnntv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dimension out layer 1: [128 , 96 , 27 , 27 ]\n",
        "*   128 samples in batch\n",
        "*   96 channels out in layer1\n",
        "*   27*27 spacial dimension of matrices after convolution"
      ],
      "metadata": {
        "id": "Ex_TpprCmEa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AFTER TRAINING RESULTS"
      ],
      "metadata": {
        "id": "uizTAQ1DgUHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  print(\"After training:\")\n",
        "  train_loss, train_accuracy = test(net, train_loader, loss_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, loss_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, loss_function)\n",
        "  print(\"\\t Training loss {:.5f}, Training accuracy {:.2f}\".format(train_loss,  train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss,  val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "Iu7jLBdtgRh1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}